{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TS-MNIST data classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit",
      "name": "python388jvsc74a57bd0f945b773209fc48410cde1e81af417b0848a0c2d85a27e647af95c6737bb9d9a",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityapribadi3/Privacy-Machine-Learning/blob/main/TS_MNIST_data_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYrLNW4TbbZK",
        "outputId": "48690e85-d5a1-4612-a34f-bf637e71886a"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(73)\n",
        "\n",
        "train_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class ConvNet(torch.nn.Module):\n",
        "    def __init__(self, hidden=64, output=10):\n",
        "        super(ConvNet, self).__init__()        \n",
        "        self.conv1 = torch.nn.Conv2d(1, 4, kernel_size=7, padding=0, stride=3)\n",
        "        self.fc1 = torch.nn.Linear(256, hidden)\n",
        "        self.fc2 = torch.nn.Linear(hidden, output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        # the model uses the square activation function\n",
        "        x = x * x\n",
        "        # flattening while keeping the batch axis\n",
        "        x = x.view(-1, 256)\n",
        "        x = self.fc1(x)\n",
        "        x = x * x\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, n_epochs=10):\n",
        "    # model in training mode\n",
        "    model.train()\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "\n",
        "        train_loss = 0.0\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # calculate average losses\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
        "    \n",
        "    # model in evaluation mode\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "model = ConvNet()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "model = train(model, train_loader, criterion, optimizer, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
            "100.0%\n",
            "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
            "102.8%\n",
            "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
            "100.0%\n",
            "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
            "112.7%\n",
            "c:\\Users\\radit\\.conda\\envs\\ts_mmist\\lib\\site-packages\\torchvision\\datasets\\mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:143.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
            "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
            "\n",
            "Processing...\n",
            "Done!\n",
            "Epoch: 1 \tTraining Loss: 0.392145\n",
            "Epoch: 2 \tTraining Loss: 0.131439\n",
            "Epoch: 3 \tTraining Loss: 0.090824\n",
            "Epoch: 4 \tTraining Loss: 0.070182\n",
            "Epoch: 5 \tTraining Loss: 0.059312\n",
            "Epoch: 6 \tTraining Loss: 0.049882\n",
            "Epoch: 7 \tTraining Loss: 0.045490\n",
            "Epoch: 8 \tTraining Loss: 0.038418\n",
            "Epoch: 9 \tTraining Loss: 0.035301\n",
            "Epoch: 10 \tTraining Loss: 0.031726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RJbOZb1bt1n",
        "outputId": "1e16dcf6-7176-445f-8950-4d16a8b24730"
      },
      "source": [
        "def test(model, test_loader, criterion):\n",
        "    # initialize lists to monitor test loss and accuracy\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "\n",
        "    # model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        test_loss += loss.item()\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare predictions to true label\n",
        "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(len(target)):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "    # calculate and print avg test loss\n",
        "    test_loss = test_loss/len(test_loader)\n",
        "    print(f'Test Loss: {test_loss:.6f}\\n')\n",
        "\n",
        "    for label in range(10):\n",
        "        print(\n",
        "            f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
        "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})'\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% ' \n",
        "        f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
        "    )\n",
        "    \n",
        "test(model, test_loader, criterion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.094794\n",
            "\n",
            "Test Accuracy of 0: 99% (974/980)\n",
            "Test Accuracy of 1: 99% (1128/1135)\n",
            "Test Accuracy of 2: 97% (1009/1032)\n",
            "Test Accuracy of 3: 97% (985/1010)\n",
            "Test Accuracy of 4: 97% (961/982)\n",
            "Test Accuracy of 5: 98% (876/892)\n",
            "Test Accuracy of 6: 98% (942/958)\n",
            "Test Accuracy of 7: 96% (992/1028)\n",
            "Test Accuracy of 8: 97% (946/974)\n",
            "Test Accuracy of 9: 97% (979/1009)\n",
            "\n",
            "Test Accuracy (Overall): 97% (9792/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLMDmeHXb046"
      },
      "source": [
        "\"\"\"\n",
        "It's a PyTorch-like model using operations implemented in TenSEAL.\n",
        "    - .mm() method is doing the vector-matrix multiplication explained above.\n",
        "    - you can use + operator to add a plain vector as a bias.\n",
        "    - .conv2d_im2col() method is doing a single convlution operation.\n",
        "    - .square_() just square the encrypted vector inplace.\n",
        "\"\"\"\n",
        "import tenseal as ts\n",
        "\n",
        "\n",
        "class EncConvNet:\n",
        "    def __init__(self, torch_nn):\n",
        "        self.conv1_weight = torch_nn.conv1.weight.data.view(\n",
        "            torch_nn.conv1.out_channels, torch_nn.conv1.kernel_size[0],\n",
        "            torch_nn.conv1.kernel_size[1]\n",
        "        ).tolist()\n",
        "        self.conv1_bias = torch_nn.conv1.bias.data.tolist()\n",
        "        \n",
        "        self.fc1_weight = torch_nn.fc1.weight.T.data.tolist()\n",
        "        self.fc1_bias = torch_nn.fc1.bias.data.tolist()\n",
        "        \n",
        "        self.fc2_weight = torch_nn.fc2.weight.T.data.tolist()\n",
        "        self.fc2_bias = torch_nn.fc2.bias.data.tolist()\n",
        "        \n",
        "        \n",
        "    def forward(self, enc_x, windows_nb):\n",
        "        # conv layer\n",
        "        enc_channels = []\n",
        "        for kernel, bias in zip(self.conv1_weight, self.conv1_bias):\n",
        "            y = enc_x.conv2d_im2col(kernel, windows_nb) + bias\n",
        "            enc_channels.append(y)\n",
        "        # pack all channels into a single flattened vector\n",
        "        enc_x = ts.CKKSVector.pack_vectors(enc_channels)\n",
        "        # square activation\n",
        "        enc_x.square_()\n",
        "        # fc1 layer\n",
        "        enc_x = enc_x.mm(self.fc1_weight) + self.fc1_bias\n",
        "        # square activation\n",
        "        enc_x.square_()\n",
        "        # fc2 layer\n",
        "        enc_x = enc_x.mm(self.fc2_weight) + self.fc2_bias\n",
        "        return enc_x\n",
        "    \n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n",
        "\n",
        "    \n",
        "def enc_test(context, model, test_loader, criterion, kernel_shape, stride):\n",
        "    # initialize lists to monitor test loss and accuracy\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "\n",
        "    for data, target in test_loader:\n",
        "        # Encoding and encryption\n",
        "        x_enc, windows_nb = ts.im2col_encoding(\n",
        "            context, data.view(28, 28).tolist(), kernel_shape[0],\n",
        "            kernel_shape[1], stride\n",
        "        )\n",
        "        # Encrypted evaluation\n",
        "        enc_output = enc_model(x_enc, windows_nb)\n",
        "        # Decryption of result\n",
        "        output = enc_output.decrypt()\n",
        "        output = torch.tensor(output).view(1, -1)\n",
        "\n",
        "        # compute loss\n",
        "        loss = criterion(output, target)\n",
        "        test_loss += loss.item()\n",
        "        \n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare predictions to true label\n",
        "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "        # calculate test accuracy for each object class\n",
        "        label = target.data[0]\n",
        "        class_correct[label] += correct.item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "\n",
        "    # calculate and print avg test loss\n",
        "    test_loss = test_loss / sum(class_total)\n",
        "    print(f'Test Loss: {test_loss:.6f}\\n')\n",
        "\n",
        "    for label in range(10):\n",
        "        print(\n",
        "            f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
        "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})'\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% ' \n",
        "        f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
        "    )\n",
        "\n",
        "\n",
        "# Load one element at a time\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\n",
        "# required for encoding\n",
        "kernel_shape = model.conv1.kernel_size\n",
        "stride = model.conv1.stride[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkDZ2upDb40h"
      },
      "source": [
        "## Encryption Parameters\n",
        "\n",
        "# controls precision of the fractional part\n",
        "bits_scale = 26\n",
        "\n",
        "# Create TenSEAL context\n",
        "context = ts.context(\n",
        "    ts.SCHEME_TYPE.CKKS,\n",
        "    poly_modulus_degree=8192,\n",
        "    coeff_mod_bit_sizes=[31, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, 31]\n",
        ")\n",
        "\n",
        "# set the scale\n",
        "context.global_scale = pow(2, bits_scale)\n",
        "\n",
        "# galois keys are required to do ciphertext rotations\n",
        "context.generate_galois_keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0PqpT8Ub9kH",
        "outputId": "97991f33-1ae6-41b9-eb85-63898f6f2cb1"
      },
      "source": [
        "enc_model = EncConvNet(model)\n",
        "enc_test(context, enc_model, test_loader, criterion, kernel_shape, stride)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.148885\n",
            "\n",
            "Test Accuracy of 0: 97% (958/980)\n",
            "Test Accuracy of 1: 99% (1128/1135)\n",
            "Test Accuracy of 2: 97% (1008/1032)\n",
            "Test Accuracy of 3: 97% (985/1010)\n",
            "Test Accuracy of 4: 97% (961/982)\n",
            "Test Accuracy of 5: 98% (876/892)\n",
            "Test Accuracy of 6: 95% (913/958)\n",
            "Test Accuracy of 7: 96% (995/1028)\n",
            "Test Accuracy of 8: 97% (945/974)\n",
            "Test Accuracy of 9: 97% (979/1009)\n",
            "\n",
            "Test Accuracy (Overall): 97% (9748/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}